model:
    num_hidden_layers: 4
    num_attention_heads: 8
    intermediate_size: 3072
    hidden_size: 768
    position_embedding_type: "absolute"
    hidden_dropout_prob: 0.0
    attention_probs_dropout_prob: 0.0
    output_hidden_states: False
    output_attentions: False
    use_cache: False

data:
    domain: "real"
    m: 7
    n: 7
    min_val: -1.0
    max_val: 1.0
    min_vocab: -10
    max_vocab: 10
    train_rank: 2
    test_rank: 2
    prec: 2
    train_p_mask: 0.3
    test_p_mask: 0.3

train:
    epochs: 50000
    num_train: 256
    num_eval: 64
    lr: 0.0001
    save_ckpt: False
    save_freq: 1000
    restore_ckpt: False
    ckpt_name: "model.pt"
wandb:
    log: True
